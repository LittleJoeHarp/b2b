{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83pOqr20AvWt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9de70a2",
        "outputId": "16f2abfe-27c5-4685-ffd5-8c46014633c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in c:\\users\\ritama\\anaconda3\\lib\\site-packages (1.36.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (1.8.2)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (4.25.8)\n",
            "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (6.4.1)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.19.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d49a6e3",
        "outputId": "e2dd22eb-20af-47e0-ba47-f1bfdb6ac88a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-04 20:38:22.041 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.041 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.055 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.056 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.057 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.059 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.062 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.062 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.074 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.075 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.078 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.079 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.080 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.080 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.083 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.085 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.087 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.088 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.089 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.090 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.090 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.091 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.091 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.108 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.108 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 20:38:22.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "\n",
        "# Assume the previous cells with AgentState, TalentScout, Onboarder, PolicyQA,\n",
        "# build_user_driven_app, etc., have been executed and their definitions are available.\n",
        "\n",
        "# --- Streamlit App ---\n",
        "\n",
        "st.set_page_config(page_title=\"HR Automation Suite\", layout=\"wide\")\n",
        "\n",
        "st.title(\"🤖 HR Automation Suite\")\n",
        "\n",
        "# Initialize session state for the workflow and its state\n",
        "if 'workflow' not in st.session_state:\n",
        "    # Assuming build_user_driven_app is defined in a previous cell\n",
        "    st.session_state.workflow = build_user_driven_app()\n",
        "\n",
        "if 'current_state' not in st.session_state:\n",
        "    st.session_state.current_state = AgentState(\n",
        "        input=\"\",\n",
        "        chat_history=[],\n",
        "        job_requirements=\"\",\n",
        "        processed_resumes=[],\n",
        "        best_candidate={},\n",
        "        onboarding_plan=\"\",\n",
        "        final_output=\"\",\n",
        "        next_action=\"ExecuteTool\", # Initial action doesn't matter much in user-driven\n",
        "        resume_directory=\"\",\n",
        "        policy_file_path=\"\"\n",
        "    )\n",
        "\n",
        "# --- Input Fields ---\n",
        "\n",
        "st.header(\"Configuration\")\n",
        "policy_file = st.text_input(\"Enter the path to your company policies file:\",\n",
        "                            value=st.session_state.current_state.get('policy_file_path', ''))\n",
        "if policy_file and os.path.isfile(policy_file):\n",
        "    st.session_state.current_state['policy_file_path'] = policy_file\n",
        "elif policy_file and not os.path.isfile(policy_file):\n",
        "    st.error(\"Invalid policy file path.\")\n",
        "\n",
        "\n",
        "# --- Tool Selection and Execution ---\n",
        "\n",
        "st.header(\"Run HR Tools\")\n",
        "\n",
        "tool_choice = st.radio(\"Choose a tool to run:\",\n",
        "                       ('TalentScout (Screen Resumes)',\n",
        "                        'Onboarder (Generate Onboarding Plan)',\n",
        "                        'PolicyQA (Answer Policy Questions)'))\n",
        "\n",
        "run_button = st.button(\"Run Selected Tool\")\n",
        "\n",
        "if run_button:\n",
        "    st.session_state.current_state['final_output'] = \"\" # Clear previous output\n",
        "\n",
        "    if tool_choice == 'TalentScout (Screen Resumes)':\n",
        "        st.session_state.current_state['next_action'] = \"TalentScout\"\n",
        "        st.session_state.current_state['resume_directory'] = st.text_input(\"Enter the path to your resume folder:\",\n",
        "                                                                        value=st.session_state.current_state.get('resume_directory', ''))\n",
        "        st.session_state.current_state['job_requirements'] = st.text_area(\"Enter the Job Requirements:\",\n",
        "                                                                       value=st.session_state.current_state.get('job_requirements', ''))\n",
        "\n",
        "        if not st.session_state.current_state['resume_directory'] or not os.path.isdir(st.session_state.current_state['resume_directory']):\n",
        "            st.error(\"Please enter a valid resume folder path.\")\n",
        "            run_button = False # Prevent execution\n",
        "        elif not st.session_state.current_state['job_requirements']:\n",
        "             st.error(\"Please enter the job requirements.\")\n",
        "             run_button = False # Prevent execution\n",
        "\n",
        "\n",
        "    elif tool_choice == 'Onboarder (Generate Onboarding Plan)':\n",
        "        st.session_state.current_state['next_action'] = \"Onboarder\"\n",
        "        if not st.session_state.current_state.get(\"best_candidate\"):\n",
        "            st.warning(\"Please run TalentScout first to select a candidate.\")\n",
        "            run_button = False # Prevent execution\n",
        "\n",
        "    elif tool_choice == 'PolicyQA (Answer Policy Questions)':\n",
        "        st.session_state.current_state['next_action'] = \"PolicyQA\"\n",
        "        st.session_state.current_state['input'] = st.text_input(\"Enter your policy question:\")\n",
        "        if not st.session_state.current_state['input']:\n",
        "             st.error(\"Please enter a policy question.\")\n",
        "             run_button = False # Prevent execution\n",
        "\n",
        "\n",
        "    if run_button: # Proceed if no validation errors\n",
        "        with st.spinner(f\"Running {st.session_state.current_state['next_action']}...\"):\n",
        "            # Execute the workflow for the selected action\n",
        "            # Need to pass the state to invoke and update the session state\n",
        "            updated_state = st.session_state.workflow.invoke(st.session_state.current_state)\n",
        "            st.session_state.current_state.update(updated_state) # Update session state with results\n",
        "\n",
        "\n",
        "# --- Display Output ---\n",
        "\n",
        "st.header(\"Output\")\n",
        "\n",
        "if st.session_state.current_state.get('final_output'):\n",
        "    st.markdown(st.session_state.current_state['final_output'])\n",
        "\n",
        "if st.session_state.current_state.get('best_candidate'):\n",
        "    st.subheader(\"Best Candidate Selected:\")\n",
        "    st.json(st.session_state.current_state['best_candidate'])\n",
        "\n",
        "if st.session_state.current_state.get('processed_resumes'):\n",
        "    st.subheader(\"Processed Resumes:\")\n",
        "    st.json(st.session_state.current_state['processed_resumes'])\n",
        "\n",
        "if st.session_state.current_state.get('onboarding_plan'):\n",
        "     st.subheader(\"Generated Onboarding Plan:\")\n",
        "     st.markdown(st.session_state.current_state['onboarding_plan'])\n",
        "\n",
        "st.subheader(\"Current State (for debugging):\")\n",
        "st.json(st.session_state.current_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bd19813"
      },
      "source": [
        "To run this Streamlit app, you will need to:\n",
        "1. Ensure all previous cells defining the workflow, agents, and tools have been executed successfully.\n",
        "2. Save the code in the cell above as a Python file (e.g., `app.py`) in your Colab environment.\n",
        "3. Open a new terminal in Colab (Terminal icon in the left sidebar).\n",
        "4. Run the command: `streamlit run app.py`\n",
        "5. Click the external URL provided by Streamlit to view your app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ0J3US0IO1l",
        "outputId": "74b956b9-6182-4ec5-8e0b-26d5eeb98442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\ritama\\anaconda3\\lib\\site-packages (3.8.7)\n",
            "Requirement already satisfied: pdfplumber in c:\\users\\ritama\\anaconda3\\lib\\site-packages (0.11.7)\n",
            "Requirement already satisfied: python-docx in c:\\users\\ritama\\anaconda3\\lib\\site-packages (1.2.0)\n",
            "Requirement already satisfied: langchain in c:\\users\\ritama\\anaconda3\\lib\\site-packages (0.2.6)\n",
            "Requirement already satisfied: langchain-google-genai in c:\\users\\ritama\\anaconda3\\lib\\site-packages (1.0.7)\n",
            "Requirement already satisfied: langgraph in c:\\users\\ritama\\anaconda3\\lib\\site-packages (0.4.5)\n",
            "Requirement already satisfied: pydantic in c:\\users\\ritama\\anaconda3\\lib\\site-packages (2.11.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (42.0.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langchain) (0.2.43)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langchain) (0.1.147)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langchain-google-genai) (0.7.1)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.26 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.1.8 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.136.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.31.0)\n",
            "Requirement already satisfied: protobuf in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.25.8)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.24.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: language-data>=1.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.26->langgraph) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.6)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: cffi>=1.12 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.63.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (2.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.62.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.1.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ritama\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy pdfplumber python-docx langchain langchain-google-genai langgraph pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3ddt47uH63V",
        "outputId": "b7cdba10-3edc-43fa-e33f-4a7f4172c5d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     --- ------------------------------------ 1.0/12.8 MB 5.6 MB/s eta 0:00:03\n",
            "     --------- ------------------------------ 2.9/12.8 MB 7.3 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 4.7/12.8 MB 7.7 MB/s eta 0:00:02\n",
            "     -------------------- ------------------- 6.6/12.8 MB 7.9 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.7/12.8 MB 8.3 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 10.7/12.8 MB 8.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.6/12.8 MB 8.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 8.5 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "rIdpiquB-duw"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import TypedDict, Annotated, List, Dict\n",
        "import operator\n",
        "import time\n",
        "import pdfplumber\n",
        "import docx\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from typing import Dict, List\n",
        "from pydantic import BaseModel\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
        "POLICY_FILE = \"/content/Company_Policies.txt\"\n",
        "\n",
        "class ParsedResume(BaseModel):\n",
        "    \"\"\"Schema for the data extracted from a resume.\"\"\"\n",
        "    name: str\n",
        "    email: str\n",
        "    mobile_number: str\n",
        "    skills: List[str]\n",
        "    education: str\n",
        "    work_experience_years: float\n",
        "    filename: str\n",
        "\n",
        "\n",
        "class SimulatedResumeParser:\n",
        "    def __init__(self, resume_path: str, skills_file: str = None, custom_regex: str = None):\n",
        "        self.resume_path = resume_path\n",
        "        self.__skills_file = skills_file\n",
        "        self.__custom_regex = custom_regex\n",
        "        self.__text = self.__extract_text(resume_path)\n",
        "\n",
        "        # Load spaCy NLP\n",
        "        self.__nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.__doc = self.__nlp(self.__text)\n",
        "        self.__matcher = Matcher(self.__nlp.vocab)\n",
        "\n",
        "        # Extract details\n",
        "        self.__details = self.__get_basic_details()\n",
        "\n",
        "    def __extract_text(self, resume_path: str) -> str:\n",
        "        \"\"\"Extract text from TXT, PDF, or DOCX files.\"\"\"\n",
        "        ext = os.path.splitext(resume_path)[1].lower()\n",
        "\n",
        "        if ext == \".pdf\":\n",
        "            text = \"\"\n",
        "            try:\n",
        "                with pdfplumber.open(resume_path) as pdf:\n",
        "                    for page in pdf.pages:\n",
        "                        text += page.extract_text() or \"\"\n",
        "                return text\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading PDF {resume_path}: {e}\")\n",
        "                return \"\"\n",
        "\n",
        "        elif ext == \".docx\":\n",
        "            text = \"\"\n",
        "            try:\n",
        "                doc = docx.Document(resume_path)\n",
        "                for para in doc.paragraphs:\n",
        "                    text += para.text + \"\\n\"\n",
        "                return text\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading DOCX {resume_path}: {e}\")\n",
        "                return \"\"\n",
        "\n",
        "        else:  # fallback for plain text\n",
        "            try:\n",
        "                with open(resume_path, 'r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    return f.read()\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading text file {resume_path}: {e}\")\n",
        "                return \"\"\n",
        "\n",
        "    def __get_basic_details(self) -> Dict:\n",
        "        \"\"\"Extract details using spaCy NLP + regex (like original ResumeParser).\"\"\"\n",
        "        text = self.__text\n",
        "        doc = self.__doc\n",
        "        details = {}\n",
        "\n",
        "        name = None\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"PERSON\":\n",
        "                name = ent.text\n",
        "                break\n",
        "        details[\"name\"] = name if name else \"Unknown Candidate\"\n",
        "\n",
        "        email = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\", text)\n",
        "        details[\"email\"] = email.group(0) if email else \"N/A\"\n",
        "\n",
        "        mobile = re.search(self.__custom_regex or r\"(\\+?\\d[\\d\\-\\s]{8,}\\d)\", text)\n",
        "        details[\"mobile_number\"] = mobile.group(0) if mobile else \"N/A\"\n",
        "\n",
        "        skill_keywords = []\n",
        "        if self.__skills_file and os.path.exists(self.__skills_file):\n",
        "            with open(self.__skills_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                skill_keywords = [line.strip() for line in f if line.strip()]\n",
        "        else:\n",
        "            skill_keywords = [\"Python\", \"SQL\", \"Spark\", \"AWS\", \"Kubernetes\",\n",
        "                              \"SEO\", \"Marketing\", \"JavaScript\", \"React\", \"Node.js\"]\n",
        "\n",
        "        found_skills = [s for s in skill_keywords if re.search(r\"\\b\" + re.escape(s) + r\"\\b\", text, re.IGNORECASE)]\n",
        "        details[\"skills\"] = found_skills\n",
        "\n",
        "        edu_keywords = [\"B.S.\", \"B.Sc\", \"M.S.\", \"M.Sc\", \"PhD\", \"Bachelor\", \"Master\", \"MBA\", \"B.Tech\", \"M.Tech\"]\n",
        "        education = None\n",
        "        for token in doc:\n",
        "            for kw in edu_keywords:\n",
        "                if kw.lower() in token.text.lower():\n",
        "                    education = token.sent.text\n",
        "                    break\n",
        "        if not education:\n",
        "            education = \"Degree Placeholder\"\n",
        "        details[\"education\"] = education\n",
        "\n",
        "        years_exp = 0.0\n",
        "        exp_match = re.search(r\"(\\d+)\\s+years\", text, re.IGNORECASE)\n",
        "        if exp_match:\n",
        "            years_exp = float(exp_match.group(1))\n",
        "        else:\n",
        "            years_exp = 1.0\n",
        "        details[\"work_experience_years\"] = years_exp\n",
        "\n",
        "        details[\"filename\"] = os.path.basename(self.resume_path)\n",
        "        return details\n",
        "\n",
        "    def get_extracted_data(self) -> ParsedResume:\n",
        "        \"\"\"Return extracted resume data as a Pydantic model.\"\"\"\n",
        "        return ParsedResume(**self.__details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arif3BWEI8df",
        "outputId": "6801245c-4bc1-45bf-a652-e6781b7722f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ERROR] Skills file not found: content/Skills.txt\n",
            "[OK] Resume folder found: content/Resume\n",
            "Files inside: ['Pratham_Thakkar.pdf', 'Praveen_kumar_T_resume.pdf', 'Vidit_Jain.pdf']\n",
            "\n",
            "🔹 Parsing file: content/Resume\\Pratham_Thakkar.pdf\n",
            "\n",
            "🔹 Parsing file: content/Resume\\Praveen_kumar_T_resume.pdf\n",
            "\n",
            "🔹 Parsing file: content/Resume\\Vidit_Jain.pdf\n",
            "\n",
            "✅ Final extracted results:\n",
            "[{'education': 'Implementedrobustfeaturesincludinguserauthenticationand • '\n",
            "               'Rated1961(CandidateMaster,top200In-\\n'\n",
            "               'registration, post management, page creation, follower system, '\n",
            "               'and dia)onCodeforces(Handle:ppt1524)and\\n'\n",
            "               'moderatorprivileges,etc. 5staronCodechef(Handle:ppt1524)\\n'\n",
            "               '• Techstackused:ReactJS|ExpressJS|NodeJS|MongoDB|Nginx •',\n",
            "  'email': 'prathampthakkar@gmail.com',\n",
            "  'filename': 'Pratham_Thakkar.pdf',\n",
            "  'mobile_number': '+918849917720',\n",
            "  'name': 'batch',\n",
            "  'skills': ['Python', 'JavaScript'],\n",
            "  'work_experience_years': 1.0},\n",
            " {'education': 'Academic details\\nB.Tech from IIT Jodhpur(2015-2019).',\n",
            "  'email': 'praveenkumart236@gmail.com',\n",
            "  'filename': 'Praveen_kumar_T_resume.pdf',\n",
            "  'mobile_number': '8310618685',\n",
            "  'name': 'Png',\n",
            "  'skills': ['Python', 'AWS'],\n",
            "  'work_experience_years': 1.0},\n",
            " {'education': 'ACHIEVEMENTS\\n'\n",
            "               '● '\n",
            "               'Codeforces(fangahawk)-2113(Master),top1.5%Globally,top75inIndia.\\n',\n",
            "  'email': 'jain.vidit@students.iiit.ac.in',\n",
            "  'filename': 'Vidit_Jain.pdf',\n",
            "  'mobile_number': 'N/A',\n",
            "  'name': 'Suresh Purini',\n",
            "  'skills': ['Python', 'SQL', 'React'],\n",
            "  'work_experience_years': 1.0}]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pprint\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    resume_folder = \"content/Resume\"\n",
        "    skills_file = \"content/Skills.txt\"\n",
        "\n",
        "    # 🔹 Check skills file\n",
        "    if os.path.exists(skills_file):\n",
        "        print(f\"[OK] Skills file found: {skills_file}\")\n",
        "        with open(skills_file, \"r\") as f:\n",
        "            print(\"Sample skills loaded:\", [line.strip() for line in f.readlines()[:5]])\n",
        "    else:\n",
        "        print(f\"[ERROR] Skills file not found: {skills_file}\")\n",
        "\n",
        "    # 🔹 Check resumes folder\n",
        "    if not os.path.exists(resume_folder):\n",
        "        print(f\"[ERROR] Resume folder not found: {resume_folder}\")\n",
        "    else:\n",
        "        print(f\"[OK] Resume folder found: {resume_folder}\")\n",
        "        print(\"Files inside:\", os.listdir(resume_folder))\n",
        "\n",
        "    results = []\n",
        "    for filename in os.listdir(resume_folder):\n",
        "        filepath = os.path.join(resume_folder, filename)\n",
        "\n",
        "        if not os.path.isfile(filepath):\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n🔹 Parsing file: {filepath}\")   # debug print\n",
        "        parser = SimulatedResumeParser(filepath, skills_file=skills_file)\n",
        "        extracted = parser.get_extracted_data()\n",
        "        results.append(extracted.dict())\n",
        "\n",
        "    print(\"\\n✅ Final extracted results:\")\n",
        "    pprint.pprint(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "d2BDrSg8_sCY"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def generate_onboarding_plan(candidate_data: dict, role: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses an LLM to create a structured 30-60-90 day onboarding plan\n",
        "    based on the candidate's skills and the specific job role.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Create a detailed, actionable 30-60-90 day onboarding plan for the new hire:\n",
        "    - Name: {candidate_data.get('name')}\n",
        "    - Skills: {', '.join(candidate_data.get('skills', []))}\n",
        "    - Target Role: {role}\n",
        "\n",
        "    Structure the output clearly with the 30, 60, and 90-day milestones.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ORG1o1FU_s27"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def process_and_score_resume(resume_filepath: str, job_requirements: str) -> dict:\n",
        "    \"\"\"\n",
        "    Parses a single resume file, extracts key details using the parser,\n",
        "    and then calculates a similarity score against the job_requirements using LLM.\n",
        "    \"\"\"\n",
        "    print(f\"  > Processing {os.path.basename(resume_filepath)}...\")\n",
        "\n",
        "    # 1. Use the simulated parser\n",
        "    parser = SimulatedResumeParser(resume_filepath)\n",
        "    parsed_data = parser.get_extracted_data().dict()\n",
        "\n",
        "    # 2. Use LLM to score the relevance\n",
        "    prompt = f\"\"\"\n",
        "    You are a professional HR screener. Score the candidate's relevance for the job\n",
        "    based on their extracted skills and experience.\n",
        "\n",
        "    JOB REQUIREMENTS: {job_requirements}\n",
        "    CANDIDATE PROFILE:\n",
        "    - Skills: {', '.join(parsed_data['skills'])}\n",
        "    - Experience: {parsed_data['work_experience_years']} years.\n",
        "    - Education: {parsed_data['education']}\n",
        "\n",
        "    Provide ONLY a single float score between 0.0 (not relevant) and 1.0 (perfect fit)\n",
        "    in your response. Do not include any other text or explanation.\n",
        "    \"\"\"\n",
        "\n",
        "    score = 0.5 # Default score if LLM extraction fails\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        # Attempt to find the score float in the response content\n",
        "        score_match = re.search(r'(\\d\\.\\d+)', response.content.strip())\n",
        "        if score_match:\n",
        "            score = float(score_match.group(1))\n",
        "    except Exception as e:\n",
        "        print(f\"LLM scoring failed, using default: {e}\")\n",
        "\n",
        "    time.sleep(0.1) # Simulate complex operation time\n",
        "\n",
        "    return {\n",
        "        \"candidate_data\": parsed_data,\n",
        "        \"relevance_score\": score,\n",
        "        \"filename\": os.path.basename(resume_filepath)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Y1m5Vn9s_z08"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def retrieve_policy_answer(question: str, policy_file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Simulates a RAG tool by searching a local policy file for an answer.\n",
        "    \"\"\"\n",
        "    print(f\"  > Searching policy documents for: '{question}'\")\n",
        "\n",
        "    try:\n",
        "        with open(policy_file_path, 'r') as f:\n",
        "            policy_text = f.read()\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: Policy documents ({policy_file_path}) were not found. Cannot answer policy questions.\"\n",
        "\n",
        "    # Use LLM to perform Q/A over the retrieved text (simulated RAG context)\n",
        "    prompt = f\"\"\"\n",
        "    Using ONLY the following policy text, answer the user's question concisely.\n",
        "    If the answer is not available in the text, state that.\n",
        "\n",
        "    POLICY TEXT:\n",
        "    ---\n",
        "    {policy_text}\n",
        "    ---\n",
        "\n",
        "    QUESTION: {question}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8WLfrVlo7hjf"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    \"\"\"Represents the state of our multi-agent system.\"\"\"\n",
        "    input: str\n",
        "    chat_history: Annotated[List[HumanMessage], operator.add]\n",
        "    job_requirements: str\n",
        "    processed_resumes: List[dict]\n",
        "    best_candidate: dict\n",
        "    onboarding_plan: str\n",
        "    final_output: str\n",
        "    next_action: str\n",
        "    resume_directory: str\n",
        "    policy_file_path: str # Added policy file path to state\n",
        "\n",
        "\n",
        "class ManagerOrchestrator:\n",
        "    \"\"\"Directs the flow based on the user's request.\"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.tool_map = {\n",
        "            \"process_and_score_resume\": \"TalentScout\",\n",
        "            \"retrieve_policy_answer\": \"PolicyQA\",\n",
        "            \"generate_onboarding_plan\": \"Onboarder\"\n",
        "        }\n",
        "        self.llm = llm.bind_tools([process_and_score_resume, retrieve_policy_answer, generate_onboarding_plan])\n",
        "\n",
        "    def route_request(self, state: AgentState):\n",
        "        \"\"\"Routes the request to the appropriate agent.\"\"\"\n",
        "\n",
        "        if state.get('next_action') == \"ONBOARD_BEST_CANDIDATE\":\n",
        "            return {\"next_action\": \"Onboarder\"}\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are the Manager/Orchestrator. Determine the user's intent:\n",
        "        1. **RESUME SCREENING**: If the user asks to \"find the best candidate\" or \"screen resumes\" (delegate to TalentScout).\n",
        "        2. **POLICY QUESTION**: If the user asks about \"policy,\" \"vacation,\" \"leave,\" \"expense\" (delegate to PolicyQA).\n",
        "        3. **ONBOARDING**: If the user asks to \"create plan\" or \"onboard\" and a candidate is already selected (delegate to Onboarder).\n",
        "\n",
        "        Current Request: {state['input']}\n",
        "\n",
        "        Delegate the task by calling the appropriate tool. If the request does not clearly fit, just output a conversational response.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            # Use tool calling to force a decision\n",
        "            response = self.llm.invoke(prompt)\n",
        "            tool_call = response.tool_calls[0]\n",
        "            tool_name = tool_call['name']\n",
        "\n",
        "            agent_role = self.tool_map.get(tool_name)\n",
        "            if agent_role:\n",
        "                print(f\"\\n[Orchestrator] ROUTING: {tool_name} -> {agent_role}\")\n",
        "                return {\"next_action\": agent_role}\n",
        "\n",
        "        except Exception:\n",
        "            # If no tool call, assume conversational response is required\n",
        "            print(\"\\n[Orchestrator] ROUTING: Conversational Response/Fallback\")\n",
        "            return {\"final_output\": f\"I'm an HR automation system. I can screen resumes, answer policy questions, and create onboarding plans. Please specify what you need.\"}\n",
        "\n",
        "\n",
        "class TalentScout:\n",
        "    \"\"\"Agent for parsing, scoring, and selecting the best resume.\"\"\"\n",
        "\n",
        "    def run_screening(self, state: AgentState):\n",
        "        \"\"\"Scans the directory and runs the scoring tool for all files.\"\"\"\n",
        "        print(f\"\\n[TalentScout] Starting screening in directory: {state['resume_directory']}\")\n",
        "\n",
        "        # Filter files to only include common resume types\n",
        "        resume_files = [os.path.join(state['resume_directory'], f)\n",
        "                        for f in os.listdir(state['resume_directory'])\n",
        "                        if f.lower().endswith(('.txt', '.pdf', '.docx'))]\n",
        "\n",
        "        if not resume_files:\n",
        "            return {\"final_output\": f\"Error: No .txt, .pdf, or .docx files found in the specified directory: {state['resume_directory']}\"}\n",
        "\n",
        "        all_results = []\n",
        "        for file_path in resume_files:\n",
        "            try:\n",
        "                # Invoke the tool for each resume\n",
        "                result = process_and_score_resume.invoke({\n",
        "                    \"resume_filepath\": file_path,\n",
        "                    \"job_requirements\": state['job_requirements']\n",
        "                })\n",
        "                all_results.append(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        # Select the best candidate\n",
        "        best_candidate = max(all_results, key=lambda x: x['relevance_score'])\n",
        "\n",
        "        output_message = (\n",
        "            f\"Screening complete! **{len(all_results)}** resumes processed.\\n\\n\"\n",
        "            f\"**Top Candidate:** {best_candidate['candidate_data']['name']}\\n\"\n",
        "            f\"**File:** {best_candidate['filename']}\\n\"\n",
        "            f\"**Relevance Score:** {best_candidate['relevance_score']:.2f}/1.00\\n\"\n",
        "            f\"**Action:** The best candidate has been selected. Do you want to proceed with **Onboarding Plan** creation?\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"processed_resumes\": all_results,\n",
        "            \"best_candidate\": best_candidate['candidate_data'],\n",
        "            \"final_output\": output_message,\n",
        "            \"next_action\": \"SCREENING_COMPLETE\"\n",
        "        }\n",
        "\n",
        "class Onboarder:\n",
        "    \"\"\"Generate onboarding plan using LLM\"\"\"\n",
        "    @staticmethod\n",
        "    def create_plan(state: AgentState):\n",
        "        candidate = state['best_candidate']\n",
        "        if not candidate:\n",
        "            return {\"final_output\": \"No best candidate selected!\", \"next_action\": \"\"}\n",
        "\n",
        "        # Read company policies text\n",
        "        try:\n",
        "            with open(state['policy_file_path'], \"r\", encoding=\"utf-8\") as f:\n",
        "                policies_text = f.read()\n",
        "        except Exception as e:\n",
        "            return {\"final_output\": f\"Error reading policy file: {e}\", \"next_action\": \"\"}\n",
        "\n",
        "        # LLM prompt\n",
        "        prompt = f\"\"\"\n",
        "You are an HR automation assistant. Generate a comprehensive onboarding plan\n",
        "for a new employee based on the following information.\n",
        "\n",
        "Candidate Details:\n",
        "- Name: {candidate['name']}\n",
        "- Email: {candidate['email']}\n",
        "- Skills: {', '.join(candidate['skills'])}\n",
        "- Education: {candidate['education']}\n",
        "- Work Experience (years): {candidate['work_experience_years']}\n",
        "\n",
        "Job Requirements:\n",
        "{state['job_requirements']}\n",
        "\n",
        "Company Policies (summarized or relevant sections):\n",
        "{policies_text[:3000]}  # use first 3000 chars to avoid hitting token limits\n",
        "\n",
        "Requirements:\n",
        "- Provide a step-by-step onboarding plan.\n",
        "- Include tasks such as document submission, orientation, setup, team introduction, and first project assignment.\n",
        "- Reference company policies where relevant.\n",
        "- Keep it professional and clear.\n",
        "\n",
        "Output the onboarding plan in readable steps.\n",
        "\"\"\"\n",
        "\n",
        "        # Call the LLM\n",
        "        try:\n",
        "            from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "            llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3)\n",
        "            response = llm.invoke(prompt)\n",
        "            plan_content = response.content # Access the content attribute\n",
        "        except Exception as e:\n",
        "            return {\"final_output\": f\"Error generating onboarding plan with LLM: {e}\", \"next_action\": \"\"}\n",
        "\n",
        "        final_message = f\"--- ✅ Onboarding Plan Generated ✅ ---\\n{plan_content}\"\n",
        "\n",
        "        return {\n",
        "            \"onboarding_plan\": plan_content,\n",
        "            \"final_output\": final_message,\n",
        "            \"next_action\": \"PolicyQA\"\n",
        "        }\n",
        "\n",
        "\n",
        "class PolicyQA:\n",
        "    \"\"\"Agent for answering policy questions using RAG.\"\"\"\n",
        "\n",
        "    def answer_question(self, state: AgentState):\n",
        "        \"\"\"Invokes the RAG tool to get the policy answer.\"\"\"\n",
        "        print(f\"\\n[PolicyQA] Answering question...\")\n",
        "\n",
        "        answer = retrieve_policy_answer.invoke({\"question\": state['input'], \"policy_file_path\": state['policy_file_path']}) # Pass policy file path\n",
        "\n",
        "        final_message = f\"--- 📄 **Policy Answer** 📄 ---\\n{answer}\"\n",
        "\n",
        "        return {\n",
        "            \"final_output\": final_message,\n",
        "            \"next_action\": \"QA_COMPLETE\"\n",
        "        }\n",
        "\n",
        "def execute_tool_call(state: AgentState):\n",
        "    \"\"\"Executes the specific agent logic determined by the Manager.\"\"\"\n",
        "    action = state['next_action']\n",
        "\n",
        "    if action == \"TalentScout\":\n",
        "        return TalentScout().run_screening(state)\n",
        "    elif action == \"PolicyQA\":\n",
        "        return PolicyQA().answer_question(state)\n",
        "    elif action == \"Onboarder\":\n",
        "        return Onboarder().create_plan(state)\n",
        "\n",
        "    return {\"final_output\": f\"Error: Unknown tool or missing logic for action: {action}\"}\n",
        "\n",
        "\n",
        "def build_app():\n",
        "    manager = ManagerOrchestrator(llm)\n",
        "    workflow = StateGraph(AgentState)\n",
        "\n",
        "    workflow.add_node(\"Manager\", manager.route_request)\n",
        "    workflow.add_node(\"ExecuteTool\", execute_tool_call)\n",
        "\n",
        "    workflow.set_entry_point(\"Manager\")\n",
        "    workflow.add_edge(\"Manager\", \"ExecuteTool\")\n",
        "\n",
        "    def decide_next_step(state: AgentState):\n",
        "        \"\"\"Conditional edge to route based on the ExecuteTool result.\"\"\"\n",
        "        action = state['next_action']\n",
        "\n",
        "        if action == \"SCREENING_COMPLETE\":\n",
        "            return \"Manager\"\n",
        "        elif action == \"QA_COMPLETE\" or action == \"ONBOARDING_COMPLETE\":\n",
        "            return END\n",
        "        else:\n",
        "            return \"Manager\"\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"ExecuteTool\",\n",
        "        decide_next_step,\n",
        "        {\"Manager\": \"Manager\", END: END}\n",
        "    )\n",
        "\n",
        "    return workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "z0KQR4yqUDH0"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "def execute_tool_call(state: AgentState):\n",
        "    \"\"\"Executes the specific agent logic based on user selection.\"\"\"\n",
        "    action = state['next_action']\n",
        "\n",
        "    if action == \"TalentScout\":\n",
        "        output = TalentScout().run_screening(state)\n",
        "        state.update(output)\n",
        "        print(\"\\n\" + state['final_output'])\n",
        "    elif action == \"Onboarder\":\n",
        "        if not state.get(\"best_candidate\"):\n",
        "            print(\"\\nError: No TalentScout results available. Run TalentScout first.\")\n",
        "            return state\n",
        "        output = Onboarder().create_plan(state)\n",
        "        state.update(output)\n",
        "        print(\"\\n\" + state['final_output'])\n",
        "    elif action == \"PolicyQA\":\n",
        "        question = input(\"\\nEnter your policy question (or type 'exit' to go back): \")\n",
        "        if question.lower() != \"exit\":\n",
        "            state['input'] = question\n",
        "            output = PolicyQA().answer_question(state)\n",
        "            print(\"\\n\" + output['final_output'])\n",
        "            state.update(output)\n",
        "    else:\n",
        "        print(f\"\\nUnknown action: {action}\")\n",
        "\n",
        "    return state\n",
        "\n",
        "def build_user_driven_app():\n",
        "    \"\"\"Builds a workflow that executes a single tool based on user input and then ends.\"\"\"\n",
        "    workflow = StateGraph(AgentState)\n",
        "\n",
        "    workflow.add_node(\"ExecuteTool\", execute_tool_call)\n",
        "    workflow.set_entry_point(\"ExecuteTool\")\n",
        "    workflow.add_edge(\"ExecuteTool\", END) # Remove conditional edge, go directly to END\n",
        "\n",
        "    return workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcpmCvnUbgOm",
        "outputId": "1204188f-ba35-4970-e5fa-189c62a3e06a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Available tools:\n",
            "1. TalentScout\n",
            "2. Onboarder\n",
            "3. PolicyQA\n",
            "4. Exit\n",
            "\n",
            "[TalentScout] Starting screening in directory: content/Resumes\n",
            "  > Processing Pratham_Thakkar.pdf...\n",
            "  > Processing Praveen_kumar_T_resume.pdf...\n",
            "  > Processing Vidit_Jain.pdf...\n",
            "\n",
            "Screening complete! **3** resumes processed.\n",
            "\n",
            "**Top Candidate:** Png\n",
            "**File:** Praveen_kumar_T_resume.pdf\n",
            "**Relevance Score:** 0.95/1.00\n",
            "**Action:** The best candidate has been selected. Do you want to proceed with **Onboarding Plan** creation?\n",
            "\n",
            "Available tools:\n",
            "1. TalentScout\n",
            "2. Onboarder\n",
            "3. PolicyQA\n",
            "4. Exit\n",
            "\n",
            "--- ✅ Onboarding Plan Generated ✅ ---\n",
            "Here is a comprehensive onboarding plan for Png, tailored to the provided candidate details and company policies.\n",
            "\n",
            "---\n",
            "\n",
            "**Onboarding Plan for Png - Entry Level Data Engineer**\n",
            "\n",
            "**Candidate Details:**\n",
            "*   **Name:** Png\n",
            "*   **Email:** praveenkumart236@gmail.com\n",
            "*   **Skills:** Python, AWS\n",
            "*   **Education:** B.Tech from IIT Jodhpur (2015-2019)\n",
            "*   **Work Experience:** 1.0 years\n",
            "*   **Job Role:** Entry Level Data Engineer (requiring Python experience)\n",
            "\n",
            "---\n",
            "\n",
            "**Phase 1: Pre-Boarding (Prior to Day 1)**\n",
            "\n",
            "**Objective:** Ensure a smooth start for Png by preparing all necessary resources and information.\n",
            "\n",
            "*   **HR Department Tasks:**\n",
            "    *   Send a formal welcome email to Png, confirming start date, time, office location, and what to expect on Day 1.\n",
            "    *   Prepare all necessary HR documents (offer letter, employment contract, benefits enrollment forms, tax forms).\n",
            "    *   Prepare the employee handbook and a digital copy of the **Company Policies** for review.\n",
            "    *   Assign an onboarding \"buddy\" or mentor from the Data Engineering team.\n",
            "*   **IT Department Tasks:**\n",
            "    *   Set up Png's workstation (laptop, monitors, peripherals).\n",
            "    *   Create all necessary accounts: company email, internal communication platforms (e.g., Slack, Teams), HR system, project management tools, and access to relevant development environments (AWS console access, code repositories).\n",
            "    *   Install essential software (IDE, Python environment, AWS CLI, etc.).\n",
            "*   **Manager (Hiring Manager) Tasks:**\n",
            "    *   Prepare a detailed Day 1 and Week 1 schedule.\n",
            "    *   Identify initial learning resources and a small, introductory project or task.\n",
            "    *   Inform the team about Png's arrival and role.\n",
            "\n",
            "---\n",
            "\n",
            "**Phase 2: Day 1 - Welcome & Foundation**\n",
            "\n",
            "**Objective:** Make Png feel welcome, complete essential administrative tasks, and provide an initial overview of the company and role.\n",
            "\n",
            "*   **Morning (9:00 AM - 12:00 PM): HR & IT Formalities**\n",
            "    *   **Welcome & Introduction:** Greet Png, introduce them to the HR representative and immediate manager.\n",
            "    *   **HR Paperwork:**\n",
            "        *   Review and sign employment contract and other HR documents.\n",
            "        *   Complete benefits enrollment and tax forms.\n",
            "        *   Provide the employee handbook and a physical copy of the **Company Policies**.\n",
            "        *   **Policy Review:** Briefly go over key sections of the **Company Policies**, specifically:\n",
            "            *   **Code of Conduct:** Emphasize professionalism, ethics, and respect.\n",
            "            *   **Working Hours and Attendance:** Clarify standard hours (9:00 AM - 6:00 PM) and reporting absences.\n",
            "            *   **Leave Policy:** Explain the process for submitting leave requests.\n",
            "            *   **Acknowledgment:** Ensure Png understands the requirement to read and acknowledge all policies.\n",
            "    *   **IT Setup & Orientation:**\n",
            "        *   Receive company laptop and equipment.\n",
            "        *   IT team assists with initial login, password setup (emphasizing **Data Security and Privacy** for secure passwords), and basic network connectivity.\n",
            "        *   Brief overview of IT support channels.\n",
            "*   **Afternoon (1:00 PM - 6:00 PM): Team & Role Introduction**\n",
            "    *   **Office Tour:** Manager or buddy provides an office tour, highlighting key areas (workstation, break rooms, restrooms, emergency exits – referencing **Health and Safety**).\n",
            "    *   **Team Introduction:** Formal introduction to the immediate Data Engineering team and other key colleagues.\n",
            "    *   **Buddy Introduction:** Introduce Png to their assigned onboarding buddy.\n",
            "    *   **Manager 1:1:**\n",
            "        *   Discuss the Data Engineering team's mission, current projects, and Png's role within the team.\n",
            "        *   Review the Day 1 and Week 1 schedule.\n",
            "        *   Set initial expectations and answer any questions.\n",
            "        *   Provide access to internal documentation and knowledge bases.\n",
            "    *   **Initial Learning:** Direct Png to start reviewing internal documentation related to Data Engineering processes and tools.\n",
            "\n",
            "---\n",
            "\n",
            "**Phase 3: Week 1 - Integration & Initial Learning**\n",
            "\n",
            "**Objective:** Help Png understand company culture, internal processes, and begin familiarizing themselves with the Data Engineering environment.\n",
            "\n",
            "*   **Company & Department Orientation:**\n",
            "    *   **Deeper Dive into Company:** Manager or HR provides a more detailed overview of the company's history, mission, values, and organizational structure.\n",
            "    *   **Department Overview:** Detailed explanation of the Data Engineering department's goals, key projects, and how it integrates with other teams.\n",
            "*   **Tool & System Familiarization:**\n",
            "    *   **Internal Tools Training:** Guided walkthrough of essential internal tools (e.g., project management software, code repositories, internal communication channels).\n",
            "    *   **AWS Environment:** Introduction to the company's AWS environment and specific services used by the Data Engineering team, leveraging Png's existing AWS skills.\n",
            "    *   **Policy Reinforcement:** Remind Png about **Use of Company Resources** regarding software, internet, and equipment, and **Data Security and Privacy** when handling company data.\n",
            "*   **Team Engagement:**\n",
            "    *   **Team Meetings:** Png attends all relevant team meetings to observe discussions and project progress.\n",
            "    *   **Shadowing:** Spend time shadowing team members to understand daily workflows, code review processes, and data pipeline operations.\n",
            "    *   **Buddy Support:** Regular check-ins with the buddy for informal questions and guidance.\n",
            "*   **First Small Task/Project:**\n",
            "    *   Assign a low-stakes, introductory task (e.g., setting up a local development environment, running a simple data query, or a small documentation update) to get hands-on experience. This helps apply Python skills immediately.\n",
            "\n",
            "---\n",
            "\n",
            "**Phase 4: First Month - Deeper Engagement & Contribution**\n",
            "\n",
            "**Objective:** Png becomes more independent, starts contributing meaningfully to projects, and receives initial performance feedback.\n",
            "\n",
            "*   **Role & Project Involvement:**\n",
            "    *   **Detailed Role Discussion:** Manager conducts a 1:1 to discuss Png's specific responsibilities, key performance indicators (KPIs), and how their work contributes to team goals (**Performance and Professional Development**).\n",
            "    *   **Project Assignment:** Assign Png to a component of an ongoing Data Engineering project, leveraging their Python and AWS skills. This could involve data ingestion, transformation, or pipeline monitoring.\n",
            "    *   **Code Review Process:** Familiarize Png with the team's code review standards and best practices.\n",
            "*   **Training & Development:**\n",
            "    *   **Mandatory Training:** Complete any outstanding compliance training (e.g., advanced **Data Security and Privacy**, **Health and Safety** protocols).\n",
            "    *   **Skill Enhancement:** Provide access to relevant online courses or internal workshops to deepen knowledge in specific Data Engineering tools or advanced Python libraries.\n",
            "*   **Feedback & Check-ins:**\n",
            "    *   **Regular 1:1s:** Weekly 1:1 meetings with the manager to discuss progress, challenges, and provide constructive feedback (**Performance and Professional Development**).\n",
            "    *   **Informal Feedback:** Encourage feedback from the buddy and other team members.\n",
            "    *   **Conflict of Interest:** Manager briefly reiterates the **Conflict of Interest** policy and encourages disclosure if any potential situations arise.\n",
            "\n",
            "---\n",
            "\n",
            "**Phase 5: First 3 Months - Growth & Performance Review**\n",
            "\n",
            "**Objective:** Png is fully integrated, contributing independently, and has a clear understanding of their performance and future development path.\n",
            "\n",
            "*   **Performance Review:**\n",
            "    *   **3-Month Performance Review:** Conduct a formal performance review with the manager. This review will assess Png's progress, contributions, adherence to company policies, and identify areas for further development (**Performance and Professional Development**).\n",
            "    *   **Goal Setting:** Collaborate with Png to set clear, measurable goals for the next 3-6 months.\n",
            "*   **Skill Development & Ownership:**\n",
            "    *   **Advanced Training:** Based on the performance review, identify specific advanced training or certifications (e.g., AWS Data Analytics Specialty) that would benefit Png's role and career growth.\n",
            "    *   **Project Ownership:** Gradually assign Png ownership of specific features or smaller data pipelines within larger projects.\n",
            "*   **Team & Company Integration:**\n",
            "    *   **Cross-functional Collaboration:** Encourage Png to proactively collaborate with other teams (e.g., Data Science, Analytics) that consume the data pipelines they work on.\n",
            "    *   **Company Events:** Encourage participation in company-wide events, social gatherings, or employee resource groups.\n",
            "\n",
            "---\n",
            "\n",
            "**Phase 6: Ongoing Support & Development**\n",
            "\n",
            "**Objective:** Ensure continuous growth, engagement, and alignment with company goals.\n",
            "\n",
            "*   **Continuous Learning:** Promote a culture of continuous learning and provide resources for skill development (**Performance and Professional Development**).\n",
            "*   **Regular Performance Reviews:** Conduct performance reviews as per company policy (e.g., bi-annual or annual).\n",
            "*   **Career Pathing:** Discuss career aspirations and potential growth paths within the company.\n",
            "*   **Policy Updates:** Ensure Png is informed of any updates to company policies via email or the HR portal (**Acknowledgment**).\n",
            "*   **Mentorship:** Maintain the buddy/mentor relationship or assign new mentors for specific skill development as needed.\n",
            "\n",
            "---\n",
            "\n",
            "Available tools:\n",
            "1. TalentScout\n",
            "2. Onboarder\n",
            "3. PolicyQA\n",
            "4. Exit\n",
            "\n",
            "[PolicyQA] Answering question...\n",
            "  > Searching policy documents for: 'Number of holidays in a month'\n",
            "\n",
            "--- 📄 **Policy Answer** 📄 ---\n",
            "The policy text does not specify the number of holidays in a month. It states that employees are entitled to annual leave, sick leave, and other statutory leaves as per local labor laws.\n",
            "\n",
            "Available tools:\n",
            "1. TalentScout\n",
            "2. Onboarder\n",
            "3. PolicyQA\n",
            "4. Exit\n",
            "\n",
            "[PolicyQA] Answering question...\n",
            "  > Searching policy documents for: 'working hours in weekday?'\n",
            "\n",
            "--- 📄 **Policy Answer** 📄 ---\n",
            "Standard working hours are Monday to Friday, 9:00 AM to 6:00 PM.\n",
            "\n",
            "Available tools:\n",
            "1. TalentScout\n",
            "2. Onboarder\n",
            "3. PolicyQA\n",
            "4. Exit\n"
          ]
        }
      ],
      "source": [
        "# Initialize state\n",
        "state = AgentState(\n",
        "    input=\"\",\n",
        "    chat_history=[],\n",
        "    job_requirements=\"Entry level Data Engineer needing Python experience\",\n",
        "    processed_resumes=[],\n",
        "    best_candidate={},\n",
        "    onboarding_plan=\"\",\n",
        "    final_output=\"\",\n",
        "    next_action=\"ExecuteTool\",\n",
        "    resume_directory=\"content/Resumes\",\n",
        "    policy_file_path=\"content/Company_Policies.txt\"\n",
        ")\n",
        "\n",
        "# Build workflow\n",
        "workflow = build_user_driven_app()\n",
        "\n",
        "# Run interactively\n",
        "while True:\n",
        "    print(\"\\nAvailable tools:\\n1. TalentScout\\n2. Onboarder\\n3. PolicyQA\\n4. Exit\")\n",
        "    choice = input(\"Which tool do you want to run? \").strip().lower()\n",
        "\n",
        "    if choice in [\"exit\", \"4\"]:\n",
        "        print(\"Exiting workflow.\")\n",
        "        break\n",
        "    elif choice in [\"talentscout\", \"1\"]:\n",
        "        state['next_action'] = \"TalentScout\"\n",
        "    elif choice in [\"onboarder\", \"2\"]:\n",
        "        state['next_action'] = \"Onboarder\"\n",
        "    elif choice in [\"policyqa\", \"3\"]:\n",
        "        state['next_action'] = \"PolicyQA\"\n",
        "    else:\n",
        "        print(\"Invalid choice, please try again.\")\n",
        "        continue\n",
        "\n",
        "    state = workflow.invoke(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKFdzX2tbjAI",
        "outputId": "1e3744c4-5da0-4595-a353-b6c7d1f71d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invalid file path. Please try again.\n",
            "Invalid file path. Please try again.\n",
            "Invalid file path. Please try again.\n",
            "Invalid file path. Please try again.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m state \u001b[38;5;241m=\u001b[39m AgentState(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     chat_history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     policy_file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     policy_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the path to your company policies file: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m policy_file \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(policy_file):\n\u001b[0;32m     17\u001b[0m         state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_file_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m policy_file\n",
            "File \u001b[1;32mc:\\Users\\Ritama\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Ritama\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "state = AgentState(\n",
        "    input=\"\",\n",
        "    chat_history=[],\n",
        "    job_requirements=\"\",\n",
        "    processed_resumes=[],\n",
        "    best_candidate={},\n",
        "    onboarding_plan=\"\",\n",
        "    final_output=\"\",\n",
        "    next_action=\"ExecuteTool\",\n",
        "    resume_directory=\"\",\n",
        "    policy_file_path=\"\"\n",
        ")\n",
        "\n",
        "while True:\n",
        "    policy_file = input(\"Enter the path to your company policies file: \").strip()\n",
        "    if policy_file and os.path.isfile(policy_file):\n",
        "        state['policy_file_path'] = policy_file\n",
        "        break\n",
        "    else:\n",
        "        print(\"Invalid file path. Please try again.\")\n",
        "\n",
        "workflow = build_user_driven_app()\n",
        "\n",
        "while True:\n",
        "    print(\"\\nAvailable tools:\\n1. TalentScout\\n2. Onboarder\\n3. PolicyQA\\n4. Exit\")\n",
        "    choice = input(\"Which tool do you want to run? \").strip().lower()\n",
        "\n",
        "    if choice in [\"exit\", \"4\"]:\n",
        "        print(\"Exiting workflow.\")\n",
        "        break\n",
        "    elif choice in [\"talentscout\", \"1\"]:\n",
        "        while True:\n",
        "            resume_folder = input(\"Enter the path to your resume folder: \").strip()\n",
        "            if resume_folder and os.path.isdir(resume_folder):\n",
        "                state['resume_directory'] = resume_folder\n",
        "                break\n",
        "            else:\n",
        "                print(\"Invalid folder path. Please try again.\")\n",
        "\n",
        "        state['job_requirements'] = input(\"Enter the Job Requirements: \").strip()\n",
        "        state['next_action'] = \"TalentScout\"\n",
        "\n",
        "    elif choice in [\"onboarder\", \"2\"]:\n",
        "        if not state.get(\"best_candidate\"):\n",
        "            print(\"No TalentScout results yet. Run TalentScout first.\")\n",
        "            continue\n",
        "        state['next_action'] = \"Onboarder\"\n",
        "\n",
        "    elif choice in [\"policyqa\", \"3\"]:\n",
        "        state['next_action'] = \"PolicyQA\"\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid choice, please try again.\")\n",
        "        continue\n",
        "\n",
        "    state = workflow.invoke(state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iadrx8imdhVY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
